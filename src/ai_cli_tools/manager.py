"""AI ëª¨ë¸ ê´€ë¦¬ ì„œë¹„ìŠ¤"""

import subprocess
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict

from ai_cli_tools.cache import CacheManager
from ai_cli_tools.constants import ALL_AI_MODELS, MIN_REVIEWERS, MODEL_CHECK_TIMEOUT
from ai_cli_tools.exceptions import NoAvailableModelsError
from ai_cli_tools.models import AIModel


class ModelManager:
    """AI ëª¨ë¸ ê°€ìš©ì„± í™•ì¸ ë° ê´€ë¦¬

    Attributes:
        cache_manager: ìºì‹œ ê´€ë¦¬ì
        available_models: ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ ë”•ì…”ë„ˆë¦¬
    """

    def __init__(self, cache_manager: CacheManager):
        """
        Args:
            cache_manager: ìºì‹œ ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤
        """
        self.cache_manager = cache_manager
        self.available_models: Dict[str, AIModel] = {}

    def check_model_availability(self, model_key: str, model: AIModel) -> bool:
        """íŠ¹ì • AI ëª¨ë¸ì˜ CLIê°€ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸

        Args:
            model_key: ëª¨ë¸ í‚¤ (ì˜ˆ: "claude")
            model: AI ëª¨ë¸ ì •ë³´

        Returns:
            ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ True, ì•„ë‹ˆë©´ False
        """
        # 1ë‹¨ê³„: CLI ì„¤ì¹˜ í™•ì¸ (ë¹ ë¥¸ ì²´í¬)
        test_cmd = model.test_command or model.command[:1] + ["--version"]

        try:
            result = subprocess.run(
                test_cmd,
                capture_output=True,
                text=True,
                timeout=MODEL_CHECK_TIMEOUT * 2,
                encoding="utf-8",
            )
            # CLIê°€ ì—†ê±°ë‚˜ ì‹¬ê°í•œ ì˜¤ë¥˜ë©´ ì¦‰ì‹œ False
            if result.returncode not in [0, 1]:
                return False
        except FileNotFoundError:
            return False
        except subprocess.TimeoutExpired:
            # íƒ€ì„ì•„ì›ƒì€ ì‹¤í–‰ì€ ë˜ì§€ë§Œ ì‘ë‹µì´ ëŠë¦° ê²½ìš°
            pass
        except Exception:
            return False

        # 2ë‹¨ê³„: ì‹¤ì œ API í˜¸ì¶œ í…ŒìŠ¤íŠ¸ (ê°„ë‹¨í•œ í˜¸ì¶œë¡œ í¬ë ˆë”§/ì¸ì¦ í™•ì¸)
        try:
            test_prompt = "ok"
            result = subprocess.run(
                model.command + [test_prompt],
                capture_output=True,
                text=True,
                timeout=10.0,  # AI API í˜¸ì¶œì€ ì¶©ë¶„í•œ ì‹œê°„ í•„ìš” (10ì´ˆ)
                encoding="utf-8",
            )

            # stdoutê³¼ stderr ëª¨ë‘ì—ì„œ ëª…í™•í•œ ì—ëŸ¬ë§Œ í™•ì¸
            output = (result.stdout + result.stderr).lower()

            # í¬ë ˆë”§/ì¸ì¦ ê´€ë ¨ ëª…í™•í•œ ì—ëŸ¬ í‚¤ì›Œë“œ
            critical_errors = [
                "doesn't have any credits",
                "purchase credits",
                "no credits",
                "credit balance",
                "billing",
                "payment required",
            ]

            # ëª…í™•í•œ í¬ë ˆë”§/ê²°ì œ ì—ëŸ¬ê°€ ìˆìœ¼ë©´ ì‚¬ìš© ë¶ˆê°€
            if any(error in output for error in critical_errors):
                return False

            # returncode 0ì´ë©´ ì„±ê³µ
            if result.returncode == 0:
                return True

            # ê·¸ ì™¸ì˜ ê²½ìš°ëŠ” CLIê°€ ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì‚¬ìš© ê°€ëŠ¥ìœ¼ë¡œ ê°„ì£¼
            # (API í‚¤ ì„¤ì • ë“±ì€ ì‚¬ìš©ìê°€ ì‹¤ì œ ì‚¬ìš© ì‹œ í•´ê²°í•  ë¬¸ì œ)
            return True

        except subprocess.TimeoutExpired:
            # íƒ€ì„ì•„ì›ƒ = APIê°€ ëŠë¦¬ì§€ë§Œ ë™ì‘í•¨ (ì‚¬ìš© ê°€ëŠ¥)
            return True
        except Exception:
            # ê¸°íƒ€ ì—ëŸ¬ëŠ” CLIê°€ ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì‚¬ìš© ê°€ëŠ¥ìœ¼ë¡œ ê°„ì£¼
            return True

    def initialize_models(self, force_refresh: bool = False) -> None:
        """ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ í™•ì¸ ë° ì´ˆê¸°í™”

        Args:
            force_refresh: Trueë©´ ìºì‹œ ë¬´ì‹œí•˜ê³  ê°•ì œë¡œ ì¬í™•ì¸

        Raises:
            NoAvailableModelsError: ìµœì†Œ ë¦¬ë·°ì–´ ìˆ˜ ë¯¸ë‹¬ ì‹œ
        """
        # ìºì‹œ í™•ì¸ (force_refreshê°€ ì•„ë‹ ë•Œë§Œ)
        if not force_refresh:
            cached_keys = self.cache_manager.load_cached_models()
            if cached_keys:
                print("âœ… ìºì‹œëœ AI ëª¨ë¸ ì •ë³´ ì‚¬ìš©")
                self.available_models = {
                    key: ALL_AI_MODELS[key] for key in cached_keys if key in ALL_AI_MODELS
                }
                if len(self.available_models) >= MIN_REVIEWERS:
                    model_names = ", ".join(m.display_name for m in self.available_models.values())
                    print(f"ğŸ¤– ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸: {model_names}\n")
                    return
                else:
                    print("âš ï¸  ìºì‹œëœ ëª¨ë¸ì´ ìµœì†Œ ìš”êµ¬ì‚¬í•­ì„ ì¶©ì¡±í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ì¬í™•ì¸í•©ë‹ˆë‹¤...\n")

        # AI ëª¨ë¸ ê°€ìš©ì„± í™•ì¸ (ë³‘ë ¬ ì²˜ë¦¬)
        print("ğŸ” AI ëª¨ë¸ ê°€ìš©ì„± í™•ì¸ ì¤‘... (ë³‘ë ¬ ì²˜ë¦¬)")
        print("-" * 60)

        # ì´ˆê¸°í™” (ì´ì „ ë°ì´í„° ì œê±°)
        self.available_models.clear()
        available_keys = []

        # ë³‘ë ¬ë¡œ ëª¨ë“  ëª¨ë¸ í™•ì¸
        with ThreadPoolExecutor(max_workers=len(ALL_AI_MODELS)) as executor:
            # ëª¨ë“  ëª¨ë¸ í™•ì¸ ì‘ì—… ì œì¶œ
            future_to_model = {
                executor.submit(self.check_model_availability, key, model): (key, model)
                for key, model in ALL_AI_MODELS.items()
            }

            # ì™„ë£Œë˜ëŠ” ëŒ€ë¡œ ê²°ê³¼ ì²˜ë¦¬
            for future in as_completed(future_to_model):
                model_key, model = future_to_model[future]
                try:
                    is_available = future.result()
                    if is_available:
                        self.available_models[model_key] = model
                        available_keys.append(model_key)
                        print(f"  âœ… {model.display_name} - ì‚¬ìš© ê°€ëŠ¥")
                    else:
                        print(f"  âŒ {model.display_name} - ì‚¬ìš© ë¶ˆê°€")
                except Exception as e:
                    print(f"  âŒ {model.display_name} - í™•ì¸ ì‹¤íŒ¨: {e}")

        print("-" * 60)

        # ìµœì†Œ ë¦¬ë·°ì–´ ìˆ˜ í™•ì¸
        if len(self.available_models) < MIN_REVIEWERS:
            error_msg = self._get_installation_guide(len(self.available_models))
            raise NoAvailableModelsError(error_msg)

        # ìºì‹œ ì €ì¥
        self.cache_manager.save_cached_models(available_keys)

        print(f"\nâœ… {len(self.available_models)}ê°œì˜ AI ë¦¬ë·°ì–´ ì‚¬ìš© ê°€ëŠ¥")
        model_names = ", ".join(m.display_name for m in self.available_models.values())
        print(f"ğŸ¤– ì‚¬ìš© ê°€ëŠ¥í•œ ë¦¬ë·°ì–´: {model_names}\n")

    def get_available_models(self) -> Dict[str, AIModel]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ë°˜í™˜

        Returns:
            ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ ë”•ì…”ë„ˆë¦¬
        """
        return self.available_models

    @staticmethod
    def _get_installation_guide(current_count: int) -> str:
        """AI CLI ì„¤ì¹˜ ì•ˆë‚´ ë©”ì‹œì§€ ìƒì„±

        Args:
            current_count: í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ AI ê°œìˆ˜

        Returns:
            ì„¤ì¹˜ ì•ˆë‚´ ë©”ì‹œì§€
        """
        return f"""
âŒ ìµœì†Œ {MIN_REVIEWERS}ê°œì˜ AI CLIê°€ í•„ìš”í•˜ì§€ë§Œ {current_count}ê°œë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤!

AI ì½”ë“œ ë¦¬ë·° ì‹œìŠ¤í…œì€ ì—¬ëŸ¬ AIê°€ ì„œë¡œë¥¼ ê²€ì¦í•˜ëŠ” ë°©ì‹ì´ë¯€ë¡œ
ìµœì†Œ {MIN_REVIEWERS}ê°œ ì´ìƒì˜ AI CLIê°€ í•„ìš”í•©ë‹ˆë‹¤.

ë‹¤ìŒ AI CLI ì¤‘ {MIN_REVIEWERS}ê°œ ì´ìƒì„ ì„¤ì¹˜í•´ì£¼ì„¸ìš”:

1. Claude (Anthropic)
   - ì„¤ì¹˜: npm install -g @anthropic-ai/claude-cli
   - ë¬¸ì„œ: https://docs.anthropic.com/claude/docs/claude-cli

2. OpenAI GPT (Codex)
   - ì„¤ì¹˜: npm install -g @openai/codex-cli
   - ë¬¸ì„œ: https://platform.openai.com/docs/codex

3. Gemini (Google)
   - ì„¤ì¹˜: pip install google-generativeai
   - ë¬¸ì„œ: https://ai.google.dev/docs

4. Grok (xAI)
   - ì„¤ì¹˜: pip install grok-cli
   - ë¬¸ì„œ: https://x.ai/docs

ğŸ’¡ ì„¤ì¹˜ í›„ ë‹¤ìŒì„ ì‹¤í–‰í•˜ì—¬ ìºì‹œë¥¼ ê°±ì‹ í•˜ì„¸ìš”:
   python ai_code_review.py --force-refresh

ë˜ëŠ” ìºì‹œ íŒŒì¼ì„ ì§ì ‘ ì‚­ì œ:
   rm .ai_code_review_cache.json
"""

# AI Code Review System - ê°œë°œ ì‹¤í–‰ ê³„íš

## ğŸ“‹ ê°œìš”

ì´ ë¬¸ì„œëŠ” AI Code Review Systemì˜ Phase 1 (MVP v1.0) ê°œë°œì„ ìœ„í•œ ìƒì„¸ ì‹¤í–‰ ê³„íšì…ë‹ˆë‹¤.
Claude-flowë¥¼ í†µí•´ ë‹¨ê³„ì ìœ¼ë¡œ êµ¬í˜„í•˜ë©°, ê° Featureë§ˆë‹¤ í…ŒìŠ¤íŠ¸ ìš°ì„  ê°œë°œ(TDD)ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

### ì°¸ê³  ë¬¸ì„œ
- **README.md**: ì‚¬ìš©ì ê°€ì´ë“œ ë° ê¸°ëŠ¥ ê°œìš”
- **CLAUDE.md**: ê¸°ìˆ  ì•„í‚¤í…ì²˜ ë° êµ¬í˜„ ìƒì„¸
- **PRD.md**: ì œí’ˆ ìš”êµ¬ì‚¬í•­ ë° ê¸°ëŠ¥ ëª…ì„¸

### ê°œë°œ ì›ì¹™
1. âœ… **í…ŒìŠ¤íŠ¸ ìš°ì„ **: ê° ê¸°ëŠ¥ êµ¬í˜„ ì „ pytest í…ŒìŠ¤íŠ¸ ì‘ì„±
2. ğŸ”„ **ì ì§„ì  êµ¬í˜„**: Feature ë‹¨ìœ„ë¡œ ê°œë°œ ë° ê²€ì¦
3. ğŸ“ **ë¬¸ì„œ ë™ê¸°í™”**: êµ¬í˜„ ì™„ë£Œ ì‹œ PRD.md ì²´í¬ë°•ìŠ¤ ì—…ë°ì´íŠ¸ í›„ ì»¤ë°‹
4. ğŸ‘¤ **ì‚¬ìš©ì í™•ì¸**: ê° Feature ì™„ë£Œ ì‹œ ì‚¬ìš©ì í™•ì¸ í•„ìˆ˜
5. ğŸ¤– **Agent í™œìš©**: ì ì ˆí•œ Agentë¥¼ ë³‘ë ¬ë¡œ í™œìš©í•˜ì—¬ íš¨ìœ¨ì„± ê·¹ëŒ€í™”

---

## ğŸ¯ Phase 1: MVP (v1.0) ê°œë°œ ê³„íš

### í˜„ì¬ ìƒíƒœ
- [x] `ai_cli_tools` ëª¨ë“ˆ ì™„ì„± (ai-discussionì—ì„œ ì¶”ì¶œ ë° ì¡°ì •)
- [x] í”„ë¡œì íŠ¸ êµ¬ì¡° ì„¤ì • (.gitignore, pyproject.toml, requirements-dev.txt)
- [x] ë¬¸ì„œ ì‘ì„± ì™„ë£Œ (README.md, CLAUDE.md, PRD.md)

### êµ¬í˜„ ëŒ€ìƒ Features

```
Phase 1 MVP
â”œâ”€â”€ F1: AI CLI ìë™ ê°ì§€ ì‹œìŠ¤í…œ âœ… (ai_cli_tools ëª¨ë“ˆë¡œ ì™„ë£Œ)
â”œâ”€â”€ F2: ë°ì´í„° ëª¨ë¸ êµ¬í˜„ (ReviewIssue, ReviewContext)
â”œâ”€â”€ F3: ë¦¬ë·° í”„ë¡œì„¸ìŠ¤ ì—”ì§„ (AICodeReviewSystem)
â”‚   â”œâ”€â”€ F3-1: Phase 1 - ë…ë¦½ì  ì´ˆê¸° ë¦¬ë·°
â”‚   â”œâ”€â”€ F3-2: Phase 2 - ë¹„íŒì  ê²€ì¦
â”‚   â””â”€â”€ F3-3: Phase 3 - ìµœì¢… í•©ì˜
â”œâ”€â”€ F4: 5ê°€ì§€ ë¦¬ë·° ëª¨ë“œ êµ¬í˜„
â”œâ”€â”€ F5: í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹œìŠ¤í…œ
â”œâ”€â”€ F6: ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ ìƒì„±
â””â”€â”€ F7: CLI ì¸í„°í˜ì´ìŠ¤ ë° í†µí•©
```

---

## ğŸ“¦ Feature 2: ë°ì´í„° ëª¨ë¸ êµ¬í˜„

### ëª©í‘œ
ReviewIssueì™€ ReviewContext ë°ì´í„° í´ë˜ìŠ¤ë¥¼ êµ¬í˜„í•˜ì—¬ ë¦¬ë·° ì‹œìŠ¤í…œì˜ í•µì‹¬ ë°ì´í„° êµ¬ì¡°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

### êµ¬í˜„ ì‚¬í•­

#### íŒŒì¼: `ai_code_review/models.py`

```python
"""ë¦¬ë·° ì‹œìŠ¤í…œ ë°ì´í„° ëª¨ë¸"""

from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional
from enum import Enum


class Severity(Enum):
    """ì´ìŠˆ ì‹¬ê°ë„"""
    CRITICAL = "CRITICAL"
    MAJOR = "MAJOR"
    MINOR = "MINOR"
    SUGGESTION = "SUGGESTION"


class ReviewMode(Enum):
    """ë¦¬ë·° ëª¨ë“œ"""
    FILE = "file"
    DIRECTORY = "directory"
    STAGED = "staged"
    COMMITS = "commits"
    BRANCH = "branch"


@dataclass
class ReviewIssue:
    """ì½”ë“œ ë¦¬ë·° ì´ìŠˆ

    Attributes:
        severity: ì‹¬ê°ë„ (CRITICAL/MAJOR/MINOR/SUGGESTION)
        title: ì´ìŠˆ ì œëª©
        location: íŒŒì¼:ë¼ì¸ í˜•ì‹ (ì˜ˆ: "main.py:45-47")
        description: ìƒì„¸ ì„¤ëª…
        code_snippet: ë¬¸ì œê°€ ë˜ëŠ” ì½”ë“œ
        suggestion: ê°œì„  ì œì•ˆ (ì½”ë“œ í¬í•¨)
        reviewer: ë°œê²¬í•œ ë¦¬ë·°ì–´ ì´ë¦„ (AI ì´ë¦„)
        verified: ë‹¤ë¥¸ ë¦¬ë·°ì–´ë“¤ì´ ê²€ì¦í–ˆëŠ”ì§€ ì—¬ë¶€
        verification_notes: ê²€ì¦ ê³¼ì • ê¸°ë¡
    """
    severity: str
    title: str
    location: str
    description: str
    code_snippet: str
    suggestion: str
    reviewer: str
    verified: bool = False
    verification_notes: List[str] = field(default_factory=list)

    def __post_init__(self):
        """ë°ì´í„° ê²€ì¦"""
        valid_severities = [s.value for s in Severity]
        if self.severity not in valid_severities:
            raise ValueError(f"Invalid severity: {self.severity}. Must be one of {valid_severities}")


@dataclass
class ReviewContext:
    """ë¦¬ë·° ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸

    Attributes:
        target_path: ë¦¬ë·° ëŒ€ìƒ ê²½ë¡œ
        review_mode: ë¦¬ë·° ëª¨ë“œ (file/directory/staged/commits/branch)
        files: ë¦¬ë·°í•  íŒŒì¼ ëª©ë¡
        mcp_context: MCPë¡œë¶€í„° ìˆ˜ì§‘í•œ ì •ë³´
        git_info: Git ê´€ë ¨ ì •ë³´
        max_rounds: ìµœëŒ€ ê²€ì¦ ë¼ìš´ë“œ
        allow_early_exit: ì¡°ê¸° ì¢…ë£Œ í—ˆìš© ì—¬ë¶€
        use_mcp: MCP ì‚¬ìš© ì—¬ë¶€
        file_extensions: í•„í„°ë§í•  í™•ì¥ì
    """
    target_path: str
    review_mode: str
    files: List[str]
    mcp_context: Dict[str, Any] = field(default_factory=dict)
    git_info: Dict[str, Any] = field(default_factory=dict)
    max_rounds: int = 3
    allow_early_exit: bool = True
    use_mcp: bool = True
    file_extensions: Optional[List[str]] = None

    def __post_init__(self):
        """ë°ì´í„° ê²€ì¦"""
        valid_modes = [m.value for m in ReviewMode]
        if self.review_mode not in valid_modes:
            raise ValueError(f"Invalid review_mode: {self.review_mode}. Must be one of {valid_modes}")

        if self.max_rounds < 1:
            raise ValueError("max_rounds must be at least 1")
```

### í…ŒìŠ¤íŠ¸ êµ¬í˜„

#### íŒŒì¼: `tests/test_models.py`

```python
"""ë°ì´í„° ëª¨ë¸ í…ŒìŠ¤íŠ¸"""

import pytest
from ai_code_review.models import ReviewIssue, ReviewContext, Severity, ReviewMode


class TestReviewIssue:
    """ReviewIssue í…ŒìŠ¤íŠ¸"""

    def test_create_valid_issue(self):
        """ì •ìƒì ì¸ ì´ìŠˆ ìƒì„±"""
        issue = ReviewIssue(
            severity="CRITICAL",
            title="SQL Injection",
            location="main.py:45",
            description="SQL ì¸ì ì…˜ ì·¨ì•½ì ",
            code_snippet="query = f'SELECT * FROM users WHERE id={user_id}'",
            suggestion="parameterized query ì‚¬ìš©",
            reviewer="claude"
        )

        assert issue.severity == "CRITICAL"
        assert issue.verified is False
        assert len(issue.verification_notes) == 0

    def test_invalid_severity(self):
        """ì˜ëª»ëœ ì‹¬ê°ë„ ì‹œ ì—ëŸ¬"""
        with pytest.raises(ValueError, match="Invalid severity"):
            ReviewIssue(
                severity="INVALID",
                title="Test",
                location="test.py:1",
                description="Test",
                code_snippet="test",
                suggestion="test",
                reviewer="test"
            )

    def test_verification_notes(self):
        """ê²€ì¦ ë…¸íŠ¸ ì¶”ê°€"""
        issue = ReviewIssue(
            severity="MAJOR",
            title="Test",
            location="test.py:1",
            description="Test",
            code_snippet="test",
            suggestion="test",
            reviewer="claude",
            verification_notes=["ê²€ì¦ 1", "ê²€ì¦ 2"]
        )

        assert len(issue.verification_notes) == 2


class TestReviewContext:
    """ReviewContext í…ŒìŠ¤íŠ¸"""

    def test_create_valid_context(self):
        """ì •ìƒì ì¸ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        context = ReviewContext(
            target_path="./src/main.py",
            review_mode="file",
            files=["./src/main.py"]
        )

        assert context.target_path == "./src/main.py"
        assert context.max_rounds == 3
        assert context.allow_early_exit is True

    def test_invalid_review_mode(self):
        """ì˜ëª»ëœ ë¦¬ë·° ëª¨ë“œ ì‹œ ì—ëŸ¬"""
        with pytest.raises(ValueError, match="Invalid review_mode"):
            ReviewContext(
                target_path="./src",
                review_mode="invalid",
                files=[]
            )

    def test_invalid_max_rounds(self):
        """ì˜ëª»ëœ max_rounds ì‹œ ì—ëŸ¬"""
        with pytest.raises(ValueError, match="max_rounds must be at least 1"):
            ReviewContext(
                target_path="./src",
                review_mode="file",
                files=[],
                max_rounds=0
            )

    def test_custom_settings(self):
        """ì»¤ìŠ¤í…€ ì„¤ì •"""
        context = ReviewContext(
            target_path="./src",
            review_mode="directory",
            files=["./src/a.py", "./src/b.py"],
            max_rounds=5,
            allow_early_exit=False,
            use_mcp=False,
            file_extensions=[".py", ".js"]
        )

        assert context.max_rounds == 5
        assert context.allow_early_exit is False
        assert context.use_mcp is False
        assert ".py" in context.file_extensions
```

### ê²€ì¦ ê¸°ì¤€
- [ ] ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼ (`uv run pytest tests/test_models.py`)
- [ ] íƒ€ì… ì²´í¬ í†µê³¼ (`uv run mypy ai_code_review/models.py`)
- [ ] Enum ì‚¬ìš©ìœ¼ë¡œ íƒ€ì… ì•ˆì „ì„± í™•ë³´
- [ ] ë°ì´í„° ê²€ì¦ ë¡œì§ ë™ì‘ í™•ì¸

### Agent í™œìš©
- **Orient Agent**: ë°ì´í„° ëª¨ë¸ ì„¤ê³„ ê²€í† 
- **Code Review Agent**: ì½”ë“œ í’ˆì§ˆ ê²€ì¦

---

## ğŸ“¦ Feature 3-1: Phase 1 - ë…ë¦½ì  ì´ˆê¸° ë¦¬ë·°

### ëª©í‘œ
ëª¨ë“  AI ë¦¬ë·°ì–´ê°€ ë³‘ë ¬ë¡œ ë…ë¦½ì ì¸ ì½”ë“œ ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” Phase 1ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

### êµ¬í˜„ ì‚¬í•­

#### íŒŒì¼: `ai_code_review/review_engine.py`

```python
"""ë¦¬ë·° ì—”ì§„ - Phase 1 êµ¬í˜„"""

from typing import Dict, List
from concurrent.futures import ThreadPoolExecutor, as_completed
from ai_cli_tools import AIClient, AIModel
from ai_code_review.models import ReviewIssue, ReviewContext
from ai_code_review.prompt_generator import PromptGenerator


class ReviewEngine:
    """ì½”ë“œ ë¦¬ë·° ì—”ì§„"""

    def __init__(
        self,
        ai_client: AIClient,
        prompt_generator: PromptGenerator
    ):
        self.ai_client = ai_client
        self.prompt_generator = prompt_generator

    def phase1_initial_review(
        self,
        context: ReviewContext,
        available_ais: Dict[str, AIModel]
    ) -> Dict[str, List[ReviewIssue]]:
        """Phase 1: ë…ë¦½ì  ì´ˆê¸° ë¦¬ë·° (ë³‘ë ¬ ì‹¤í–‰)

        Args:
            context: ë¦¬ë·° ì»¨í…ìŠ¤íŠ¸
            available_ais: ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ë“¤

        Returns:
            {ai_name: [issues]} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
        """
        print("\n[Phase 1] ë…ë¦½ì  ì´ˆê¸° ë¦¬ë·° ì‹œì‘...")
        reviews = {}

        # ì½”ë“œ ì½ê¸°
        code_content = self._read_files(context.files)

        # ë³‘ë ¬ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=len(available_ais)) as executor:
            futures = {}

            for ai_name, ai_model in available_ais.items():
                # í”„ë¡¬í”„íŠ¸ ìƒì„±
                prompt = self.prompt_generator.generate_initial_review_prompt(
                    context=context,
                    code_content=code_content,
                    ai_name=ai_name
                )

                # Agent ì§€ì •
                agents = ["Explore", "Observe", "Orient", "Security", "Performance"]

                # ë¹„ë™ê¸° ì‹¤í–‰
                future = executor.submit(
                    self.ai_client.call_ai_with_retry,
                    prompt,
                    ai_model,
                    agents
                )
                futures[future] = ai_name

            # ê²°ê³¼ ìˆ˜ì§‘
            for future in as_completed(futures):
                ai_name = futures[future]
                try:
                    response = future.result(timeout=600)
                    issues = self._parse_review_response(response, ai_name)
                    reviews[ai_name] = issues
                    print(f"  âœ“ {ai_name}: {len(issues)}ê°œ ì´ìŠˆ ë°œê²¬")
                except Exception as e:
                    print(f"  âœ— {ai_name}: ë¦¬ë·° ì‹¤íŒ¨ - {e}")
                    reviews[ai_name] = []

        return reviews

    def _read_files(self, files: List[str]) -> Dict[str, str]:
        """íŒŒì¼ë“¤ ì½ê¸°"""
        content = {}
        for file_path in files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content[file_path] = f.read()
            except Exception as e:
                print(f"âš ï¸  íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {file_path}: {e}")
                content[file_path] = ""
        return content

    def _parse_review_response(
        self,
        response: str,
        reviewer: str
    ) -> List[ReviewIssue]:
        """AI ì‘ë‹µì„ ReviewIssue ë¦¬ìŠ¤íŠ¸ë¡œ íŒŒì‹±

        ì‘ë‹µ í˜•ì‹:
        [SEVERITY] ì´ìŠˆ ì œëª©
        - ìœ„ì¹˜: íŒŒì¼:ë¼ì¸
        - ì„¤ëª…: ...
        - ì½”ë“œ: ...
        - ì œì•ˆ: ...
        """
        issues = []
        # TODO: íŒŒì‹± ë¡œì§ êµ¬í˜„
        # ì •ê·œì‹ì´ë‚˜ êµ¬ì¡°í™”ëœ íŒŒì‹± í•„ìš”
        return issues
```

### í…ŒìŠ¤íŠ¸ êµ¬í˜„

#### íŒŒì¼: `tests/test_review_engine_phase1.py`

```python
"""ReviewEngine Phase 1 í…ŒìŠ¤íŠ¸"""

import pytest
from unittest.mock import Mock, patch
from ai_code_review.review_engine import ReviewEngine
from ai_code_review.models import ReviewContext
from ai_cli_tools import AIClient, AIModel


@pytest.fixture
def mock_ai_client():
    """Mock AI í´ë¼ì´ì–¸íŠ¸"""
    client = Mock(spec=AIClient)
    client.call_ai_with_retry.return_value = """
[CRITICAL] SQL Injection
- ìœ„ì¹˜: test.py:10
- ì„¤ëª…: SQL ì¸ì ì…˜ ì·¨ì•½ì 
- ì½”ë“œ: query = f"SELECT * FROM users WHERE id={user_id}"
- ì œì•ˆ: parameterized query ì‚¬ìš©
"""
    return client


@pytest.fixture
def mock_prompt_generator():
    """Mock í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸°"""
    generator = Mock()
    generator.generate_initial_review_prompt.return_value = "Review this code..."
    return generator


@pytest.fixture
def review_engine(mock_ai_client, mock_prompt_generator):
    """ReviewEngine ì¸ìŠ¤í„´ìŠ¤"""
    return ReviewEngine(mock_ai_client, mock_prompt_generator)


def test_phase1_initial_review_parallel_execution(review_engine):
    """Phase 1 ë³‘ë ¬ ì‹¤í–‰ í…ŒìŠ¤íŠ¸"""
    context = ReviewContext(
        target_path="./test.py",
        review_mode="file",
        files=["./tests/fixtures/sample.py"]
    )

    available_ais = {
        "claude": AIModel("Claude", ["claude", "-p"], "Claude (Anthropic)"),
        "gemini": AIModel("Gemini", ["gemini", "-p"], "Gemini (Google)")
    }

    # íŒŒì¼ ì½ê¸° Mock
    with patch.object(review_engine, '_read_files') as mock_read:
        mock_read.return_value = {"./tests/fixtures/sample.py": "def hello(): pass"}

        reviews = review_engine.phase1_initial_review(context, available_ais)

    # ëª¨ë“  AIê°€ í˜¸ì¶œë˜ì—ˆëŠ”ì§€ í™•ì¸
    assert len(reviews) == 2
    assert "claude" in reviews
    assert "gemini" in reviews


def test_phase1_handles_ai_failure(review_engine, mock_ai_client):
    """AI ì‹¤íŒ¨ ì‹œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
    # í•˜ë‚˜ì˜ AIëŠ” ì‹¤íŒ¨
    mock_ai_client.call_ai_with_retry.side_effect = [
        "Valid response",
        Exception("API Error")
    ]

    context = ReviewContext(
        target_path="./test.py",
        review_mode="file",
        files=["./tests/fixtures/sample.py"]
    )

    available_ais = {
        "claude": AIModel("Claude", ["claude", "-p"], "Claude"),
        "gemini": AIModel("Gemini", ["gemini", "-p"], "Gemini")
    }

    with patch.object(review_engine, '_read_files') as mock_read:
        mock_read.return_value = {"test.py": "code"}

        reviews = review_engine.phase1_initial_review(context, available_ais)

    # ì„±ê³µí•œ AIì˜ ë¦¬ë·°ë§Œ í¬í•¨
    assert len(reviews) == 2
    assert reviews["gemini"] == []  # ì‹¤íŒ¨í•œ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸
```

### ê²€ì¦ ê¸°ì¤€
- [ ] ë³‘ë ¬ ì‹¤í–‰ ë™ì‘ í™•ì¸
- [ ] AI í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ graceful degradation
- [ ] í”„ë¡¬í”„íŠ¸ ìƒì„± ë° Agent ì§€ì • í™•ì¸
- [ ] ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼

### Agent í™œìš©
- **Backend Developer Agent**: ë¦¬ë·° ì—”ì§„ êµ¬í˜„
- **Test Engineer**: í…ŒìŠ¤íŠ¸ ì½”ë“œ ì‘ì„±

---

## ğŸ“¦ Feature 3-2: Phase 2 - ë¹„íŒì  ê²€ì¦

### ëª©í‘œ
ê° AIê°€ ë‹¤ë¥¸ AIì˜ ë¦¬ë·°ë¥¼ ë¹„íŒì ìœ¼ë¡œ ê²€ì¦í•˜ëŠ” Phase 2ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

### êµ¬í˜„ ì‚¬í•­

#### íŒŒì¼: `ai_code_review/review_engine.py` (ì¶”ê°€)

```python
def phase2_critical_verification(
    self,
    context: ReviewContext,
    initial_reviews: Dict[str, List[ReviewIssue]],
    available_ais: Dict[str, AIModel]
) -> List[Dict]:
    """Phase 2: ë¹„íŒì  ê²€ì¦ (ìˆœì°¨ ë¼ìš´ë“œ)

    Args:
        context: ë¦¬ë·° ì»¨í…ìŠ¤íŠ¸
        initial_reviews: Phase 1 ê²°ê³¼
        available_ais: ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ë“¤

    Returns:
        ë¼ìš´ë“œë³„ ê²€ì¦ ê¸°ë¡
    """
    print("\n[Phase 2] ë¹„íŒì  ê²€ì¦ ì‹œì‘...")
    verification_history = []

    for round_num in range(1, context.max_rounds + 1):
        print(f"\n  Round {round_num}/{context.max_rounds}")
        round_verifications = {}

        for ai_name, ai_model in available_ais.items():
            # ìì‹ ì„ ì œì™¸í•œ ë‹¤ë¥¸ AIë“¤ì˜ ë¦¬ë·°
            other_reviews = {
                name: issues
                for name, issues in initial_reviews.items()
                if name != ai_name
            }

            # ê²€ì¦ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self.prompt_generator.generate_verification_prompt(
                ai_name=ai_name,
                own_reviews=initial_reviews.get(ai_name, []),
                other_reviews=other_reviews,
                round_num=round_num
            )

            # Agent ì§€ì •
            agents = ["Explore", "Observe", "Orient"]

            # AI í˜¸ì¶œ
            try:
                response = self.ai_client.call_ai_with_retry(
                    prompt,
                    ai_model,
                    agents
                )

                verification = self._parse_verification_response(response)
                round_verifications[ai_name] = verification

                # ê²€ì¦ ê²°ê³¼ë¥¼ ì›ë³¸ ì´ìŠˆì— ë°˜ì˜
                self._apply_verification_results(initial_reviews, verification)

                print(f"    âœ“ {ai_name}: ê²€ì¦ ì™„ë£Œ")
            except Exception as e:
                print(f"    âœ— {ai_name}: ê²€ì¦ ì‹¤íŒ¨ - {e}")

        verification_history.append({
            "round": round_num,
            "verifications": round_verifications
        })

        # ì¡°ê¸° ì¢…ë£Œ ì²´í¬ (ìµœì†Œ 2ë¼ìš´ë“œ í›„)
        if round_num >= 2 and context.allow_early_exit:
            if self._check_all_consensus_ready(available_ais, verification_history):
                print(f"\n  âœ“ ëª¨ë“  ë¦¬ë·°ì–´ê°€ í•©ì˜ ì¤€ë¹„ ì™„ë£Œ (Round {round_num})")
                break

    return verification_history

def _parse_verification_response(self, response: str) -> Dict:
    """ê²€ì¦ ì‘ë‹µ íŒŒì‹±"""
    # TODO: êµ¬í˜„
    return {
        "agreed": [],
        "disagreed": [],
        "severity_adjustments": []
    }

def _apply_verification_results(
    self,
    reviews: Dict[str, List[ReviewIssue]],
    verification: Dict
) -> None:
    """ê²€ì¦ ê²°ê³¼ë¥¼ ì´ìŠˆì— ë°˜ì˜"""
    # TODO: êµ¬í˜„
    pass

def _check_all_consensus_ready(
    self,
    available_ais: Dict[str, AIModel],
    history: List[Dict]
) -> bool:
    """ëª¨ë“  AIê°€ í•©ì˜ ì¤€ë¹„ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
    # TODO: ê° AIì—ê²Œ YES/NO ì§ˆë¬¸
    return False
```

### í…ŒìŠ¤íŠ¸ êµ¬í˜„

#### íŒŒì¼: `tests/test_review_engine_phase2.py`

```python
"""ReviewEngine Phase 2 í…ŒìŠ¤íŠ¸"""

import pytest
from ai_code_review.review_engine import ReviewEngine
from ai_code_review.models import ReviewIssue, ReviewContext


def test_phase2_verification_rounds():
    """ê²€ì¦ ë¼ìš´ë“œ ì‹¤í–‰ í…ŒìŠ¤íŠ¸"""
    # TODO: êµ¬í˜„
    pass


def test_phase2_early_exit():
    """ì¡°ê¸° ì¢…ë£Œ í…ŒìŠ¤íŠ¸"""
    # TODO: êµ¬í˜„
    pass


def test_phase2_verification_application():
    """ê²€ì¦ ê²°ê³¼ ë°˜ì˜ í…ŒìŠ¤íŠ¸"""
    # TODO: êµ¬í˜„
    pass
```

### ê²€ì¦ ê¸°ì¤€
- [ ] ìˆœì°¨ ë¼ìš´ë“œ ì‹¤í–‰ í™•ì¸
- [ ] ê²€ì¦ ê²°ê³¼ íŒŒì‹± ë° ë°˜ì˜
- [ ] ì¡°ê¸° ì¢…ë£Œ ë¡œì§ ë™ì‘
- [ ] ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼

---

## ğŸ“¦ Feature 3-3: Phase 3 - ìµœì¢… í•©ì˜

### ëª©í‘œ
ê²€ì¦ëœ ì´ìŠˆë“¤ì„ í†µí•©í•˜ì—¬ ìµœì¢… í•©ì˜ ë¦¬ë·°ë¥¼ ìƒì„±í•˜ëŠ” Phase 3ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

### êµ¬í˜„ ì‚¬í•­

#### íŒŒì¼: `ai_code_review/review_engine.py` (ì¶”ê°€)

```python
def phase3_final_consensus(
    self,
    context: ReviewContext,
    initial_reviews: Dict[str, List[ReviewIssue]],
    verification_history: List[Dict],
    available_ais: Dict[str, AIModel]
) -> Dict[str, Any]:
    """Phase 3: ìµœì¢… í•©ì˜ ìƒì„±

    Args:
        context: ë¦¬ë·° ì»¨í…ìŠ¤íŠ¸
        initial_reviews: Phase 1 ê²°ê³¼
        verification_history: Phase 2 ê²€ì¦ ê¸°ë¡
        available_ais: ì‚¬ìš© ê°€ëŠ¥í•œ AI ëª¨ë¸ë“¤

    Returns:
        í†µí•©ëœ ìµœì¢… ë¦¬ë·°
    """
    print("\n[Phase 3] ìµœì¢… í•©ì˜ ìƒì„± ì¤‘...")

    # 1. ê²€ì¦ëœ ì´ìŠˆë§Œ í•„í„°ë§
    verified_issues = self._filter_verified_issues(initial_reviews)

    # 2. ìœ ì‚¬ ì´ìŠˆ í†µí•©
    merged_issues = self._merge_similar_issues(verified_issues)

    # 3. ìš°ì„ ìˆœìœ„ ì •ë ¬
    sorted_issues = self._sort_by_priority(merged_issues)

    # 4. í†µí•© ìš”ì•½ ìƒì„± (AI í™œìš©)
    summary = self._generate_summary(sorted_issues, available_ais)

    # 5. í†µê³„ ìƒì„±
    statistics = self._calculate_statistics(sorted_issues)

    final_review = {
        "summary": summary,
        "issues": sorted_issues,
        "statistics": statistics,
        "context": context,
        "verification_history": verification_history
    }

    print("  âœ“ í†µí•© ë¦¬ë·° ë¬¸ì„œ ìƒì„± ì™„ë£Œ")

    return final_review
```

### ê²€ì¦ ê¸°ì¤€
- [ ] ê²€ì¦ëœ ì´ìŠˆë§Œ í¬í•¨
- [ ] ìœ ì‚¬ ì´ìŠˆ í†µí•© ë¡œì§ ë™ì‘
- [ ] ìš°ì„ ìˆœìœ„ ì •ë ¬ ì •í™•ì„±
- [ ] í†µí•© ìš”ì•½ ìƒì„±
- [ ] ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼

---

## ğŸ“¦ Feature 4: 5ê°€ì§€ ë¦¬ë·° ëª¨ë“œ

### ëª©í‘œ
íŒŒì¼, ë””ë ‰í† ë¦¬, staged, commits, branch 5ê°€ì§€ ë¦¬ë·° ëª¨ë“œë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

### êµ¬í˜„ ì‚¬í•­

#### íŒŒì¼: `ai_code_review/file_analyzer.py`

```python
"""íŒŒì¼ ë¶„ì„ ë° Git í†µí•©"""

from typing import List, Dict, Any
from pathlib import Path
import subprocess
from ai_code_review.models import ReviewContext, ReviewMode


class FileAnalyzer:
    """íŒŒì¼ ë° Git ë¶„ì„"""

    def analyze_target(self, context: ReviewContext) -> None:
        """ë¦¬ë·° ëŒ€ìƒ ë¶„ì„ ë° íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘"""
        if context.review_mode == ReviewMode.FILE.value:
            self._analyze_file(context)
        elif context.review_mode == ReviewMode.DIRECTORY.value:
            self._analyze_directory(context)
        elif context.review_mode == ReviewMode.STAGED.value:
            self._analyze_staged(context)
        elif context.review_mode == ReviewMode.COMMITS.value:
            self._analyze_commits(context)
        elif context.review_mode == ReviewMode.BRANCH.value:
            self._analyze_branch(context)

    def _analyze_file(self, context: ReviewContext) -> None:
        """ë‹¨ì¼ íŒŒì¼ ë¶„ì„"""
        if Path(context.target_path).is_file():
            context.files = [context.target_path]
        else:
            raise FileNotFoundError(f"File not found: {context.target_path}")

    def _analyze_directory(self, context: ReviewContext) -> None:
        """ë””ë ‰í† ë¦¬ ë¶„ì„"""
        path = Path(context.target_path)
        if not path.is_dir():
            raise NotADirectoryError(f"Not a directory: {context.target_path}")

        # íŒŒì¼ ìˆ˜ì§‘ (í™•ì¥ì í•„í„°ë§)
        files = []
        for file_path in path.rglob("*"):
            if file_path.is_file():
                if self._should_include_file(file_path, context.file_extensions):
                    files.append(str(file_path))

        context.files = files

    def _analyze_staged(self, context: ReviewContext) -> None:
        """Staged ë³€ê²½ì‚¬í•­ ë¶„ì„"""
        try:
            # git diff --cached --name-only
            result = subprocess.run(
                ["git", "diff", "--cached", "--name-only"],
                capture_output=True,
                text=True,
                check=True
            )

            files = [f.strip() for f in result.stdout.split('\n') if f.strip()]
            context.files = files
            context.git_info["mode"] = "staged"
        except subprocess.CalledProcessError as e:
            raise RuntimeError(f"Git staged ë¶„ì„ ì‹¤íŒ¨: {e}")

    def _analyze_commits(self, context: ReviewContext) -> None:
        """ì»¤ë°‹ ë²”ìœ„ ë¶„ì„"""
        # TODO: git diff <range> --name-only
        pass

    def _analyze_branch(self, context: ReviewContext) -> None:
        """ë¸Œëœì¹˜ ë³€ê²½ì‚¬í•­ ë¶„ì„"""
        # TODO: git diff <base>...<current> --name-only
        pass

    def _should_include_file(
        self,
        file_path: Path,
        extensions: List[str] = None
    ) -> bool:
        """íŒŒì¼ í¬í•¨ ì—¬ë¶€ í™•ì¸"""
        if extensions is None:
            return True
        return file_path.suffix in extensions
```

### í…ŒìŠ¤íŠ¸ êµ¬í˜„

í…ŒìŠ¤íŠ¸ëŠ” ê° ë¦¬ë·° ëª¨ë“œë³„ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.

### ê²€ì¦ ê¸°ì¤€
- [ ] 5ê°€ì§€ ë¦¬ë·° ëª¨ë“œ ëª¨ë‘ ë™ì‘
- [ ] Git ëª…ë ¹ ì •í™•ì„±
- [ ] íŒŒì¼ í•„í„°ë§ ì •í™•ì„±
- [ ] ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼

---

## ğŸ“¦ Feature 5: í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹œìŠ¤í…œ

### ëª©í‘œ
ê° Phaseë³„ë¡œ AIì—ê²Œ ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬í˜„í•©ë‹ˆë‹¤.

### êµ¬í˜„ ì‚¬í•­

#### íŒŒì¼: `ai_code_review/prompt_generator.py`

```python
"""í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹œìŠ¤í…œ"""

from typing import Dict, List, Any
from ai_code_review.models import ReviewIssue, ReviewContext


class PromptGenerator:
    """AI í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸°"""

    def generate_initial_review_prompt(
        self,
        context: ReviewContext,
        code_content: Dict[str, str],
        ai_name: str
    ) -> str:
        """Phase 1 ì´ˆê¸° ë¦¬ë·° í”„ë¡¬í”„íŠ¸"""
        # TODO: CLAUDE.mdì˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ êµ¬í˜„
        pass

    def generate_verification_prompt(
        self,
        ai_name: str,
        own_reviews: List[ReviewIssue],
        other_reviews: Dict[str, List[ReviewIssue]],
        round_num: int
    ) -> str:
        """Phase 2 ê²€ì¦ í”„ë¡¬í”„íŠ¸"""
        # TODO: CLAUDE.mdì˜ ê²€ì¦ í”„ë¡¬í”„íŠ¸ êµ¬í˜„
        pass

    def generate_consensus_prompt(
        self,
        all_reviews: Dict[str, List[ReviewIssue]],
        verification_history: List[Dict]
    ) -> str:
        """Phase 3 í•©ì˜ í”„ë¡¬í”„íŠ¸"""
        # TODO: CLAUDE.mdì˜ í•©ì˜ í”„ë¡¬í”„íŠ¸ êµ¬í˜„
        pass
```

### ê²€ì¦ ê¸°ì¤€
- [ ] í”„ë¡¬í”„íŠ¸ì— Agent ì‚¬ìš© ì§€ì‹œ í¬í•¨
- [ ] MCP ì»¨í…ìŠ¤íŠ¸ í¬í•¨
- [ ] ëª…í™•í•œ ì¶œë ¥ í˜•ì‹ ì§€ì‹œ
- [ ] ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼

---

## ğŸ“¦ Feature 6: ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ ìƒì„±

### ëª©í‘œ
ë¦¬ë·° ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬í˜„í•©ë‹ˆë‹¤.

### êµ¬í˜„ ì‚¬í•­

#### íŒŒì¼: `ai_code_review/markdown_generator.py`

```python
"""ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ ìƒì„±"""

from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List
from ai_code_review.models import ReviewIssue, ReviewContext


class MarkdownGenerator:
    """ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ ìƒì„±ê¸°"""

    def save_review_files(
        self,
        context: ReviewContext,
        initial_reviews: Dict[str, List[ReviewIssue]],
        verification_history: List[Dict],
        final_review: Dict[str, Any]
    ) -> tuple[str, str]:
        """ë¦¬ë·° ë¬¸ì„œ ì €ì¥

        Returns:
            (ì „ì²´ ë¦¬ë·° ê²½ë¡œ, ìµœì¢… ë¦¬ë·° ê²½ë¡œ)
        """
        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        base_name = self._get_base_filename(context.target_path)

        # 1. ì „ì²´ ë¦¬ë·° ê¸°ë¡
        full_path = f"{base_name}-review-{timestamp}.md"
        with open(full_path, 'w', encoding='utf-8') as f:
            content = self._format_full_review(
                context,
                initial_reviews,
                verification_history,
                final_review,
                timestamp
            )
            f.write(content)

        # 2. ìµœì¢… í†µí•© ë¦¬ë·°
        final_path = f"{base_name}-final-review-{timestamp}.md"
        with open(final_path, 'w', encoding='utf-8') as f:
            content = self._format_final_review(
                context,
                final_review,
                timestamp
            )
            f.write(content)

        return (full_path, final_path)

    def _format_full_review(self, ...) -> str:
        """ì „ì²´ ë¦¬ë·° ë§ˆí¬ë‹¤ìš´ ìƒì„±"""
        # TODO: README.mdì˜ ì˜ˆì‹œ ì°¸ì¡°
        pass

    def _format_final_review(self, ...) -> str:
        """ìµœì¢… ë¦¬ë·° ë§ˆí¬ë‹¤ìš´ ìƒì„±"""
        # TODO: README.mdì˜ ì˜ˆì‹œ ì°¸ì¡°
        pass
```

### ê²€ì¦ ê¸°ì¤€
- [ ] 2ê°œ íŒŒì¼ ì •ìƒ ìƒì„±
- [ ] ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ì •í™•ì„±
- [ ] ì½”ë“œ ìŠ¤ë‹ˆí« í¬í•¨
- [ ] ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼

---

## ğŸ“¦ Feature 7: CLI ì¸í„°í˜ì´ìŠ¤ ë° í†µí•©

### ëª©í‘œ
argparseë¥¼ ì‚¬ìš©í•œ CLI ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬í˜„í•˜ê³  ëª¨ë“  ì»´í¬ë„ŒíŠ¸ë¥¼ í†µí•©í•©ë‹ˆë‹¤.

### êµ¬í˜„ ì‚¬í•­

#### íŒŒì¼: `ai_code_review.py`

```python
"""AI Code Review System - ë©”ì¸ ì§„ì…ì """

import argparse
import sys
from pathlib import Path
from ai_cli_tools import AIClient, ModelManager, CacheManager
from ai_cli_tools.constants import CACHE_FILE
from ai_code_review.models import ReviewContext
from ai_code_review.review_engine import ReviewEngine
from ai_code_review.file_analyzer import FileAnalyzer
from ai_code_review.prompt_generator import PromptGenerator
from ai_code_review.markdown_generator import MarkdownGenerator


def parse_arguments() -> argparse.Namespace:
    """ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(
        description="AI Code Review System - ë‹¤ì¤‘ AI ì½”ë“œ ë¦¬ë·°",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    # ë¦¬ë·° ëŒ€ìƒ
    parser.add_argument(
        "target",
        nargs="?",
        help="ë¦¬ë·°í•  íŒŒì¼ ë˜ëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ"
    )

    # ë¦¬ë·° ëª¨ë“œ
    mode_group = parser.add_mutually_exclusive_group()
    mode_group.add_argument("--staged", action="store_true")
    mode_group.add_argument("--commits", metavar="RANGE")
    mode_group.add_argument("--branch", action="store_true")

    # ì˜µì…˜
    parser.add_argument("--max-rounds", type=int, default=3)
    parser.add_argument("--only", metavar="AI_LIST")
    parser.add_argument("--no-mcp", action="store_true")
    parser.add_argument("--extensions", metavar="EXT_LIST")
    parser.add_argument("--no-early-exit", action="store_true")
    parser.add_argument("--force-refresh", action="store_true")
    parser.add_argument("--verbose", action="store_true")

    return parser.parse_args()


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    args = parse_arguments()

    # ë°°ë„ˆ ì¶œë ¥
    print_banner()

    try:
        # 1. AI CLI ì´ˆê¸°í™”
        cache_manager = CacheManager(CACHE_FILE)
        model_manager = ModelManager(cache_manager)
        model_manager.initialize_models(force_refresh=args.force_refresh)
        available_ais = model_manager.get_available_models()

        # 2. AI í•„í„°ë§ (--only ì˜µì…˜)
        if args.only:
            specified = set(args.only.split(","))
            available_ais = {k: v for k, v in available_ais.items() if k in specified}

        # 3. ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context = create_review_context(args)

        # 4. íŒŒì¼ ë¶„ì„
        file_analyzer = FileAnalyzer()
        file_analyzer.analyze_target(context)

        # 5. ë¦¬ë·° ì‹¤í–‰
        ai_client = AIClient()
        prompt_generator = PromptGenerator()
        review_engine = ReviewEngine(ai_client, prompt_generator)

        initial_reviews = review_engine.phase1_initial_review(context, available_ais)
        verification_history = review_engine.phase2_critical_verification(
            context,
            initial_reviews,
            available_ais
        )
        final_review = review_engine.phase3_final_consensus(
            context,
            initial_reviews,
            verification_history,
            available_ais
        )

        # 6. ë¬¸ì„œ ì €ì¥
        markdown_gen = MarkdownGenerator()
        full_path, final_path = markdown_gen.save_review_files(
            context,
            initial_reviews,
            verification_history,
            final_review
        )

        # 7. ì„±ê³µ ë©”ì‹œì§€
        print_success(full_path, final_path, final_review)

    except KeyboardInterrupt:
        print("\n\në¦¬ë·°ê°€ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
```

### ê²€ì¦ ê¸°ì¤€
- [ ] ëª¨ë“  ëª…ë ¹ì¤„ ì˜µì…˜ ë™ì‘
- [ ] ì—ëŸ¬ ì²˜ë¦¬ ì •ìƒ ë™ì‘
- [ ] ì „ì²´ í”Œë¡œìš° í†µí•© í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] ì‹¤ì œ AI CLIë¡œ E2E í…ŒìŠ¤íŠ¸ ì„±ê³µ

---

## ğŸ”„ ê°œë°œ ì›Œí¬í”Œë¡œìš°

### ê° Feature ê°œë°œ ìˆœì„œ

1. **í…ŒìŠ¤íŠ¸ ì‘ì„±** (TDD)
   ```bash
   # í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
   vi tests/test_<feature>.py

   # ì‹¤íŒ¨ í™•ì¸
   uv run pytest tests/test_<feature>.py
   ```

2. **êµ¬í˜„**
   ```bash
   # ì½”ë“œ ì‘ì„±
   vi ai_code_review/<module>.py

   # í…ŒìŠ¤íŠ¸ í†µê³¼ í™•ì¸
   uv run pytest tests/test_<feature>.py
   ```

3. **í’ˆì§ˆ ê²€ì¦**
   ```bash
   # íƒ€ì… ì²´í¬
   uv run mypy ai_code_review/<module>.py

   # í¬ë§· ì²´í¬
   uv run black --check ai_code_review/<module>.py

   # Lint
   uv run ruff check ai_code_review/<module>.py
   ```

4. **PRD ì—…ë°ì´íŠ¸ ë° ì»¤ë°‹**
   ```bash
   # PRD.mdì—ì„œ ì²´í¬ë°•ìŠ¤ ì—…ë°ì´íŠ¸
   vi PRD.md  # - [x] Feature X ì™„ë£Œ

   # ì»¤ë°‹
   git add .
   git commit -m "feat: Feature X êµ¬í˜„ ì™„ë£Œ

   - í…ŒìŠ¤íŠ¸ í†µê³¼: test_<feature>.py
   - íƒ€ì… ì²´í¬ í†µê³¼
   - PRD.md ì—…ë°ì´íŠ¸

   Closes #<issue_number>"
   ```

5. **ì‚¬ìš©ì í™•ì¸ ëŒ€ê¸°**

---

## ğŸ“‹ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 1 MVP ì™„ë£Œ ì¡°ê±´

#### ê¸°ëŠ¥ êµ¬í˜„
- [x] F1: AI CLI ìë™ ê°ì§€ (`ai_cli_tools` ì™„ë£Œ)
- [ ] F2: ë°ì´í„° ëª¨ë¸ (ReviewIssue, ReviewContext)
- [ ] F3-1: Phase 1 - ë…ë¦½ì  ì´ˆê¸° ë¦¬ë·°
- [ ] F3-2: Phase 2 - ë¹„íŒì  ê²€ì¦
- [ ] F3-3: Phase 3 - ìµœì¢… í•©ì˜
- [ ] F4: 5ê°€ì§€ ë¦¬ë·° ëª¨ë“œ
- [ ] F5: í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹œìŠ¤í…œ
- [ ] F6: ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ ìƒì„±
- [ ] F7: CLI ì¸í„°í˜ì´ìŠ¤ ë° í†µí•©

#### í…ŒìŠ¤íŠ¸
- [ ] ëª¨ë“  ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] E2E í…ŒìŠ¤íŠ¸ í†µê³¼ (ì‹¤ì œ AI CLI ì‚¬ìš©)
- [ ] í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ 80% ì´ìƒ

#### í’ˆì§ˆ
- [ ] íƒ€ì… íŒíŠ¸ 100%
- [ ] Docstring ëª¨ë“  ê³µê°œ API
- [ ] ì½”ë“œ í¬ë§· (black) í†µê³¼
- [ ] Lint (ruff) í†µê³¼

#### ë¬¸ì„œ
- [x] README.md ì‘ì„± ì™„ë£Œ
- [x] CLAUDE.md ì‘ì„± ì™„ë£Œ
- [x] PRD.md ì‘ì„± ì™„ë£Œ
- [ ] ëª¨ë“  ì²´í¬ë°•ìŠ¤ ì—…ë°ì´íŠ¸

---

## ğŸ¤– Agent í™œìš© ì „ëµ

### Featureë³„ Agent ë§¤í•‘

| Feature | ì£¼ Agent | ë³´ì¡° Agent |
|---------|----------|-----------|
| F2: ë°ì´í„° ëª¨ë¸ | Orient Agent | Code Review Agent |
| F3-1: Phase 1 | Backend Developer | Test Engineer |
| F3-2: Phase 2 | Backend Developer | Security Agent |
| F3-3: Phase 3 | Backend Developer | Orient Agent |
| F4: ë¦¬ë·° ëª¨ë“œ | Backend Developer | - |
| F5: í”„ë¡¬í”„íŠ¸ | Technical Writer | Orient Agent |
| F6: ë§ˆí¬ë‹¤ìš´ | Frontend Developer | Technical Writer |
| F7: CLI í†µí•© | Fullstack Developer | - |

### ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥ ì‘ì—…

- F2ì™€ F4ëŠ” ë…ë¦½ì ì´ë¯€ë¡œ ë³‘ë ¬ ê°€ëŠ¥
- F5ì™€ F6ëŠ” ë…ë¦½ì ì´ë¯€ë¡œ ë³‘ë ¬ ê°€ëŠ¥
- í…ŒìŠ¤íŠ¸ ì‘ì„±ê³¼ ë¬¸ì„œ ì‘ì„±ì€ ë³‘ë ¬ ê°€ëŠ¥

---

## ğŸ“ ì»¤ë°‹ ë©”ì‹œì§€ ê·œì¹™

```
<type>: <subject>

<body>

<footer>
```

### Type
- `feat`: ìƒˆë¡œìš´ ê¸°ëŠ¥
- `fix`: ë²„ê·¸ ìˆ˜ì •
- `test`: í…ŒìŠ¤íŠ¸ ì¶”ê°€/ìˆ˜ì •
- `docs`: ë¬¸ì„œ ìˆ˜ì •
- `refactor`: ë¦¬íŒ©í† ë§
- `style`: ì½”ë“œ í¬ë§· ë³€ê²½

### ì˜ˆì‹œ
```
feat: ReviewIssue ë° ReviewContext ë°ì´í„° ëª¨ë¸ êµ¬í˜„

- Enumì„ ì‚¬ìš©í•œ íƒ€ì… ì•ˆì „ì„± í™•ë³´
- ë°ì´í„° ê²€ì¦ ë¡œì§ ì¶”ê°€
- í…ŒìŠ¤íŠ¸ ì „ì²´ í†µê³¼ (tests/test_models.py)
- íƒ€ì… ì²´í¬ í†µê³¼

Closes #2
```

---

## ğŸš€ ì‹œì‘í•˜ê¸°

```bash
# 1. ê°œë°œ í™˜ê²½ ì„¤ì •
uv venv
source .venv/bin/activate
uv pip install -r requirements-dev.txt

# 2. ì²« ë²ˆì§¸ Feature ì‹œì‘
# Feature 2: ë°ì´í„° ëª¨ë¸ êµ¬í˜„
vi tests/test_models.py  # í…ŒìŠ¤íŠ¸ ë¨¼ì € ì‘ì„±
uv run pytest tests/test_models.py  # ì‹¤íŒ¨ í™•ì¸

mkdir -p ai_code_review
vi ai_code_review/models.py  # êµ¬í˜„
uv run pytest tests/test_models.py  # í†µê³¼ í™•ì¸

# 3. PRD ì—…ë°ì´íŠ¸ ë° ì»¤ë°‹
vi PRD.md  # Feature 2 ì²´í¬
git add .
git commit -m "feat: ReviewIssue ë° ReviewContext êµ¬í˜„"

# 4. ì‚¬ìš©ìì—ê²Œ í™•ì¸ ìš”ì²­
```

---

**ë‹¤ìŒ ë‹¨ê³„**: Feature 2 (ë°ì´í„° ëª¨ë¸) êµ¬í˜„ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.

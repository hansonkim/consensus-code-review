# Code Review Response Optimization Requirements

## ğŸ“‹ ë¬¸ì œ ì •ì˜

### í˜„ì¬ ë¬¸ì œ
- **`review_run_code_review`** ë° **`review_audit_code_review`** MCP ë„êµ¬ì˜ ì‘ë‹µ í¬ê¸°ê°€ **34,780 í† í°**ìœ¼ë¡œ MCP ì œí•œ(25,000 í† í°) ì´ˆê³¼
- ì‘ë‹µì—ëŠ” 3ê°œ AI Ã— 3 ë¼ìš´ë“œ Ã— ê° ë¦¬ë·° ë‚´ìš© = **ì „ì²´ í˜‘ì—… ëŒ€í™” íŠ¸ëœìŠ¤í¬ë¦½íŠ¸** í¬í•¨
- Claude Codeê°€ ì‘ë‹µì„ ë°›ì„ ìˆ˜ ì—†ì–´ í›„ì† ì‘ì—… ë¶ˆê°€ëŠ¥

### ì˜í–¥ë°›ëŠ” MCP ë„êµ¬
1. **`review_run_code_review`**: Claude Codeê°€ ì´ˆê¸° ë¦¬ë·° ì‘ì„± â†’ ë‹¤ë¥¸ AIë“¤ì´ ê²€í†  â†’ ë°˜ë³µ ê°œì„ 
2. **`review_audit_code_review`**: ì‚¬ìš©ìê°€ ì‘ì„±í•œ ë¦¬ë·° â†’ ë‹¤ë¥¸ AIë“¤ì´ ê²€í†  â†’ ë°˜ë³µ ê°œì„ 

**ë‘ ë„êµ¬ ëª¨ë‘ ë™ì¼í•œ ë¬¸ì œ ë°œìƒ â†’ ë™ì¼í•œ í•´ê²°ì±… ì ìš©**

### ëª©í‘œ
1. MCP ì‘ë‹µ í¬ê¸°ë¥¼ 25,000 í† í° ì´í•˜ë¡œ ì œí•œ
2. Claudeê°€ ìµœì¢… ë¦¬ë·° ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš© ê°€ëŠ¥
3. ì „ì²´ ìƒì„¸ ë‚´ìš©ì€ íŒŒì¼ë¡œ ë³´ì¡´
4. ì‚¬ìš©ìê°€ í•„ìš”ì‹œ ìƒì„¸ ë‚´ìš© ì ‘ê·¼ ê°€ëŠ¥
5. **ë‘ ë„êµ¬ê°€ ë™ì¼í•œ ì‘ë‹µ êµ¬ì¡° ì‚¬ìš©** (ì½”ë“œ ì¬ì‚¬ìš©)

---

## ğŸ¯ í•´ê²° ë°©ì•ˆ: í•˜ì´ë¸Œë¦¬ë“œ ì‘ë‹µ êµ¬ì¡°

### í•µì‹¬ ì„¤ê³„ ì›ì¹™
1. **ì¸ë¼ì¸ ìš”ì•½**: Claudeê°€ ì¦‰ì‹œ ë¶„ì„í•  ìˆ˜ ìˆëŠ” ìµœì¢… ë¦¬ë·° (< 5,000 í† í°)
2. **íŒŒì¼ ì €ì¥**: ì „ì²´ íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ ë° ë¼ìš´ë“œë³„ ìƒì„¸ ë‚´ìš©
3. **ë©”íƒ€ë°ì´í„°**: í†µê³„ ë° íŒŒì¼ ê²½ë¡œ ì •ë³´

---

## ğŸ“ API ìŠ¤í™

### 1. `review_run_code_review` ìˆ˜ì •

#### ê¸°ì¡´ ì‹œê·¸ë‹ˆì²˜
```python
async def review_run_code_review(
    base: str,
    target: str,
    max_rounds: int = 3,
    ais: str = "gpt4,gemini"
) -> str  # ì „ì²´ í…ìŠ¤íŠ¸ (ë¬¸ì œ ë°œìƒ)
```

#### ìƒˆë¡œìš´ ì‹œê·¸ë‹ˆì²˜
```python
async def review_run_code_review(
    base: str,
    target: str,
    max_rounds: int = 3,
    ais: str = "gpt4,gemini",
    verbosity: str = "summary"  # "summary" | "detailed" | "full"
) -> ReviewResponse
```

### 2. `review_audit_code_review` ìˆ˜ì •

#### ê¸°ì¡´ ì‹œê·¸ë‹ˆì²˜
```python
async def review_audit_code_review(
    base: str,
    target: str,
    initial_review: str,  # ì‚¬ìš©ìê°€ ì‘ì„±í•œ ì´ˆê¸° ë¦¬ë·°
    max_rounds: int = 3,
    ais: str = "gpt4,gemini"
) -> str  # ì „ì²´ í…ìŠ¤íŠ¸ (ë¬¸ì œ ë°œìƒ)
```

#### ìƒˆë¡œìš´ ì‹œê·¸ë‹ˆì²˜
```python
async def review_audit_code_review(
    base: str,
    target: str,
    initial_review: str,  # ì‚¬ìš©ìê°€ ì‘ì„±í•œ ì´ˆê¸° ë¦¬ë·°
    max_rounds: int = 3,
    ais: str = "gpt4,gemini",
    verbosity: str = "summary"  # "summary" | "detailed" | "full"
) -> ReviewResponse  # run_code_reviewì™€ ë™ì¼í•œ êµ¬ì¡°
```

**ì¤‘ìš”**: ë‘ ë„êµ¬ ëª¨ë‘ ë™ì¼í•œ `ReviewResponse` íƒ€ì…ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

### 3. ReviewResponse êµ¬ì¡° (ê³µí†µ)

#### ReviewResponse
```python
class ReviewResponse(TypedDict):
    session_id: str
    status: str  # "COMPLETED" | "IN_PROGRESS" | "FAILED"
    consensus: ConsensusResult
    summary: ReviewSummary
    final_review_text: str  # ìµœì¢… í•©ì˜ ë¦¬ë·° (< 5000 í† í°)
    artifacts: ArtifactPaths
```

#### ConsensusResult
```python
class ConsensusResult(TypedDict):
    result: str  # "APPROVED" | "APPROVE_WITH_CHANGES" | "REJECTED" | "NO_CONSENSUS"
    confidence: float  # 0.0 ~ 1.0
    participating_ais: list[str]  # ["claude-sonnet-4", "gpt-4", "gemini-pro"]
    rounds_completed: int
```

#### ReviewSummary
```python
class ReviewSummary(TypedDict):
    critical_issues: int
    high_priority: int
    medium_priority: int
    low_priority: int
    suggestions: int
    key_findings: list[str]  # ìµœëŒ€ 10ê°œ
    files_reviewed: int
    total_changes: int
```

#### ArtifactPaths
```python
class ArtifactPaths(TypedDict):
    summary_file: str  # "/docs/reviews/{target}-{timestamp}/summary.md"
    full_transcript: str  # "/docs/reviews/{target}-{timestamp}/full-transcript.md"
    rounds_dir: str  # "/docs/reviews/{target}-{timestamp}/rounds/"
    consensus_log: str  # "/docs/reviews/{target}-{timestamp}/consensus.json"
```

---

## ğŸ“‚ íŒŒì¼ êµ¬ì¡°

### run_code_review (Claudeê°€ ì´ˆê¸° ë¦¬ë·° ì‘ì„±)
```
/docs/reviews/{target-branch}-{timestamp}/
â”œâ”€â”€ summary.md                    # ìµœì¢… í•©ì˜ ë¦¬ë·° (Claude ì½ê¸° ê°€ëŠ¥)
â”œâ”€â”€ full-transcript.md            # ì „ì²´ ëŒ€í™” íŠ¸ëœìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ consensus.json                # í•©ì˜ ë©”íƒ€ë°ì´í„°
â”œâ”€â”€ statistics.json               # í†µê³„ ì •ë³´
â”œâ”€â”€ review-type.txt               # "run_code_review"
â””â”€â”€ rounds/
    â”œâ”€â”€ round-1-claude-initial.md
    â”œâ”€â”€ round-1-gpt4-feedback.md
    â”œâ”€â”€ round-1-gemini-feedback.md
    â”œâ”€â”€ round-2-claude-revised.md
    â”œâ”€â”€ round-2-gpt4-feedback.md
    â”œâ”€â”€ round-2-gemini-feedback.md
    â”œâ”€â”€ round-3-claude-final.md
    â”œâ”€â”€ round-3-gpt4-final.md
    â””â”€â”€ round-3-gemini-final.md
```

### audit_code_review (ì‚¬ìš©ìê°€ ì´ˆê¸° ë¦¬ë·° ì‘ì„±)
```
/docs/reviews/{target-branch}-{timestamp}/
â”œâ”€â”€ summary.md                    # ìµœì¢… í•©ì˜ ë¦¬ë·° (Claude ì½ê¸° ê°€ëŠ¥)
â”œâ”€â”€ full-transcript.md            # ì „ì²´ ëŒ€í™” íŠ¸ëœìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ consensus.json                # í•©ì˜ ë©”íƒ€ë°ì´í„°
â”œâ”€â”€ statistics.json               # í†µê³„ ì •ë³´
â”œâ”€â”€ review-type.txt               # "audit_code_review"
â”œâ”€â”€ initial-review.md             # ì‚¬ìš©ìê°€ ì‘ì„±í•œ ì´ˆê¸° ë¦¬ë·°
â””â”€â”€ rounds/
    â”œâ”€â”€ round-1-gpt4-feedback.md
    â”œâ”€â”€ round-1-gemini-feedback.md
    â”œâ”€â”€ round-2-user-revised.md    # ì‚¬ìš©ìê°€ ê°œì„ í•œ ë¦¬ë·°
    â”œâ”€â”€ round-2-gpt4-feedback.md
    â”œâ”€â”€ round-2-gemini-feedback.md
    â”œâ”€â”€ round-3-user-final.md
    â”œâ”€â”€ round-3-gpt4-final.md
    â””â”€â”€ round-3-gemini-final.md
```

**ì°¨ì´ì **:
- `run_code_review`: Claudeê°€ ë¼ìš´ë“œë§ˆë‹¤ ë¦¬ë·° ì‘ì„±
- `audit_code_review`: ì‚¬ìš©ì ë¦¬ë·°ë¥¼ AIë“¤ì´ ê²€í† ë§Œ ìˆ˜í–‰
- ë‘ ê²½ìš° ëª¨ë‘ **ë™ì¼í•œ íŒŒì¼ êµ¬ì¡° ë° ì‘ë‹µ í˜•ì‹** ì‚¬ìš©

---

## ğŸ“ íŒŒì¼ í¬ë§·

### summary.md (run_code_review)
```markdown
# Code Review Summary: {target-branch}

**Branch**: `{base}...{target}`
**Date**: {timestamp}
**Review Type**: Initial review by Claude Code
**Consensus**: {result}
**Confidence**: {confidence}%
**Reviewed by**: Claude Sonnet 4 (primary), GPT-4, Gemini Pro

## Critical Issues (3)

### ğŸ”´ SQL Injection Vulnerability in Authentication
**File**: `src/auth.py:45`
**Severity**: CRITICAL
**Description**: User input directly concatenated into SQL query...
**Recommendation**: Use parameterized queries...

## High Priority (12)

### ğŸŸ  Type Safety Issues
**Files**: `src/models/*.py`
**Description**: Missing type annotations...

## Key Findings

1. **Security**: SQL injection vulnerability requires immediate attention
2. **Type Safety**: 23 type errors across 8 files
3. **Testing**: Coverage dropped from 87% to 62%
4. **Performance**: N+1 query issue in user listing endpoint

## Recommendations

1. [ ] Fix SQL injection in `src/auth.py:45`
2. [ ] Add type annotations to models
3. [ ] Restore test coverage to >80%
4. [ ] Optimize database queries

---

*Full transcript available at: `{full-transcript-path}`*
```

### summary.md (audit_code_review)
```markdown
# Code Review Audit Summary: {target-branch}

**Branch**: `{base}...{target}`
**Date**: {timestamp}
**Review Type**: User review audit
**Initial Review**: Provided by user
**Consensus**: {result}
**Confidence**: {confidence}%
**Audited by**: GPT-4, Gemini Pro

## Initial Review Assessment

**Original Quality Score**: 7.5/10
**Improved to**: 9.2/10
**Key Improvements Made**:
- Added security considerations
- Enhanced type safety recommendations
- Included performance metrics

## Critical Issues (3)

### ğŸ”´ SQL Injection Vulnerability in Authentication
**File**: `src/auth.py:45`
**Severity**: CRITICAL
**Original Review**: âŒ Not mentioned
**Auditors Found**: âœ… Identified by GPT-4 and Gemini
**Description**: User input directly concatenated into SQL query...
**Recommendation**: Use parameterized queries...

## Audit Findings

### Issues Added by Auditors
1. **Security gaps**: 3 critical issues not in original review
2. **Type safety**: Additional 5 type errors identified
3. **Performance**: N+1 query issue missed

### Original Review Strengths
1. Good coverage of code style issues
2. Clear explanation of refactoring needs
3. Well-structured recommendations

---

*Original review available at: `{initial-review-path}`*
*Full audit transcript at: `{full-transcript-path}`*
```

### consensus.json
```json
{
  "session_id": "review-abc123",
  "timestamp": "2025-01-01T14:30:00Z",
  "review_type": "run_code_review",
  "branches": {
    "base": "develop",
    "target": "refactor/remove-unused-phase2"
  },
  "consensus": {
    "result": "APPROVE_WITH_CHANGES",
    "confidence": 0.95,
    "rounds": 3
  },
  "ais": [
    {
      "name": "claude-sonnet-4",
      "role": "primary_reviewer",
      "rounds_participated": 3
    },
    {
      "name": "gpt-4",
      "role": "validator",
      "rounds_participated": 3
    },
    {
      "name": "gemini-pro",
      "role": "validator",
      "rounds_participated": 3
    }
  ],
  "issues": {
    "critical": 3,
    "high": 12,
    "medium": 25,
    "low": 7,
    "suggestions": 8
  },
  "files_changed": 23,
  "total_changes": 456
}
```

---

## ğŸ”§ êµ¬í˜„ ìš”êµ¬ì‚¬í•­

### Phase 1: Core Response Structure (ìš°ì„ ìˆœìœ„: ë†’ìŒ)

#### 1.1 ReviewResponse í´ë˜ìŠ¤ êµ¬í˜„
**íŒŒì¼**: `src/mcp/types.py`
```python
from typing import TypedDict, Literal

class ConsensusResult(TypedDict):
    result: Literal["APPROVED", "APPROVE_WITH_CHANGES", "REJECTED", "NO_CONSENSUS"]
    confidence: float
    participating_ais: list[str]
    rounds_completed: int

class ReviewSummary(TypedDict):
    critical_issues: int
    high_priority: int
    medium_priority: int
    low_priority: int
    suggestions: int
    key_findings: list[str]
    files_reviewed: int
    total_changes: int

class ArtifactPaths(TypedDict):
    summary_file: str
    full_transcript: str
    rounds_dir: str
    consensus_log: str

class ReviewResponse(TypedDict):
    session_id: str
    status: Literal["COMPLETED", "IN_PROGRESS", "FAILED"]
    consensus: ConsensusResult
    summary: ReviewSummary
    final_review_text: str
    artifacts: ArtifactPaths
```

#### 1.2 ì‘ë‹µ ìƒì„± ë¡œì§ (ê³µí†µ)
**íŒŒì¼**: `src/mcp/handlers/review_handler.py`
```python
async def create_review_response(
    session: ReviewSession,
    verbosity: str = "summary"
) -> ReviewResponse:
    """
    ì „ì²´ ë¦¬ë·° ì„¸ì…˜ ê²°ê³¼ë¥¼ í•˜ì´ë¸Œë¦¬ë“œ ì‘ë‹µìœ¼ë¡œ ë³€í™˜

    âœ… run_code_reviewì™€ audit_code_review ëª¨ë‘ ì‚¬ìš©

    Args:
        session: ì™„ë£Œëœ ë¦¬ë·° ì„¸ì…˜
        verbosity: "summary" | "detailed" | "full"

    Returns:
        ReviewResponse with inline summary + file artifacts
    """
    # 1. íŒŒì¼ ì €ì¥
    artifact_paths = await save_review_artifacts(session)

    # 2. ìš”ì•½ ì¶”ì¶œ
    summary = extract_summary(session)
    consensus = extract_consensus(session)
    final_review = extract_final_review(session, max_tokens=5000)

    # 3. ì‘ë‹µ êµ¬ì„±
    return ReviewResponse(
        session_id=session.id,
        status="COMPLETED",
        consensus=consensus,
        summary=summary,
        final_review_text=final_review,
        artifacts=artifact_paths
    )
```

#### 1.3 MCP í•¸ë“¤ëŸ¬ í†µí•©
**íŒŒì¼**: `src/mcp/server.py`
```python
@server.call_tool()
async def review_run_code_review(
    base: str,
    target: str,
    max_rounds: int = 3,
    ais: str = "gpt4,gemini",
    verbosity: str = "summary"
) -> ReviewResponse:
    """Claude Codeê°€ ì´ˆê¸° ë¦¬ë·° ì‘ì„± â†’ ë‹¤ë¥¸ AI ê²€í† """
    session = await execute_full_review(
        base=base,
        target=target,
        initial_reviewer="claude",  # Claudeê°€ ì‘ì„±
        max_rounds=max_rounds,
        validators=ais.split(",")
    )
    return await create_review_response(session, verbosity)

@server.call_tool()
async def review_audit_code_review(
    base: str,
    target: str,
    initial_review: str,
    max_rounds: int = 3,
    ais: str = "gpt4,gemini",
    verbosity: str = "summary"
) -> ReviewResponse:
    """ì‚¬ìš©ìê°€ ì‘ì„±í•œ ë¦¬ë·° â†’ ë‹¤ë¥¸ AI ê²€í† """
    session = await execute_audit_review(
        base=base,
        target=target,
        initial_review=initial_review,  # ì‚¬ìš©ìê°€ ì‘ì„±
        max_rounds=max_rounds,
        validators=ais.split(",")
    )
    return await create_review_response(session, verbosity)  # ë™ì¼í•œ í•¨ìˆ˜ ì‚¬ìš©
```

### Phase 2: File Artifact Generation (ìš°ì„ ìˆœìœ„: ë†’ìŒ)

#### 2.1 Artifact ì €ì¥ í•¨ìˆ˜ (ê³µí†µ)
**íŒŒì¼**: `src/mcp/utils/artifact_writer.py`
```python
async def save_review_artifacts(
    session: ReviewSession
) -> ArtifactPaths:
    """
    ë¦¬ë·° ì„¸ì…˜ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥

    âœ… run_code_reviewì™€ audit_code_review ëª¨ë‘ ì‚¬ìš©

    Directory structure:
    /docs/reviews/{target}-{timestamp}/
        â”œâ”€â”€ summary.md
        â”œâ”€â”€ full-transcript.md
        â”œâ”€â”€ consensus.json
        â”œâ”€â”€ statistics.json
        â”œâ”€â”€ review-type.txt           # "run" or "audit"
        â”œâ”€â”€ initial-review.md          # auditë§Œ í•´ë‹¹
        â””â”€â”€ rounds/
            â”œâ”€â”€ round-1-claude-initial.md
            â”œâ”€â”€ round-1-gpt4-feedback.md
            â””â”€â”€ ...
    """
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    base_dir = f"/docs/reviews/{session.target_branch}-{timestamp}"

    # 1. ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(f"{base_dir}/rounds", exist_ok=True)

    # 2. Review type ê¸°ë¡
    await write_review_type(session, base_dir)

    # 3. Initial review ì €ì¥ (audit_code_reviewë§Œ í•´ë‹¹)
    if session.review_type == "audit" and session.initial_review:
        await write_initial_review(session, base_dir)

    # 4. Summary ì‘ì„±
    summary_path = await write_summary_md(session, base_dir)

    # 5. Full transcript ì‘ì„±
    transcript_path = await write_full_transcript(session, base_dir)

    # 6. Rounds ì‘ì„±
    rounds_dir = await write_round_files(session, base_dir)

    # 7. Consensus JSON ì‘ì„±
    consensus_path = await write_consensus_json(session, base_dir)

    return ArtifactPaths(
        summary_file=summary_path,
        full_transcript=transcript_path,
        rounds_dir=rounds_dir,
        consensus_log=consensus_path
    )
```

#### 2.2 Summary ìƒì„± (ê³µí†µ)
**íŒŒì¼**: `src/mcp/utils/summary_generator.py`
```python
async def write_summary_md(
    session: ReviewSession,
    base_dir: str
) -> str:
    """
    ìµœì¢… í•©ì˜ ë¦¬ë·°ë¥¼ summary.mdë¡œ ì‘ì„±

    âœ… run_code_reviewì™€ audit_code_review ëª¨ë‘ ì‚¬ìš©

    ëª©í‘œ: < 5000 í† í°, Claudeê°€ ì½ê¸° ì¢‹ì€ í˜•ì‹
    """
    # 1. ì´ìŠˆ ë¶„ë¥˜ ë° ì •ë ¬
    issues = classify_issues(session.final_review)

    # 2. Markdown ìƒì„± (review typeì— ë”°ë¼ í…œí”Œë¦¿ ì„ íƒ)
    if session.review_type == "run":
        md_content = generate_run_summary_markdown(
            session=session,
            issues=issues,
            max_length=5000
        )
    else:  # audit
        md_content = generate_audit_summary_markdown(
            session=session,
            issues=issues,
            original_review=session.initial_review,
            max_length=5000
        )

    # 3. íŒŒì¼ ì‘ì„±
    path = f"{base_dir}/summary.md"
    async with aiofiles.open(path, "w") as f:
        await f.write(md_content)

    return path
```

### Phase 3: Verbosity Modes (ìš°ì„ ìˆœìœ„: ì¤‘ê°„)

#### 3.1 Verbosity ì²˜ë¦¬
```python
if verbosity == "summary":
    # ìµœì†Œ ì‘ë‹µ: ìš”ì•½ + íŒŒì¼ ê²½ë¡œë§Œ
    final_review_text = extract_final_review(session, max_tokens=5000)

elif verbosity == "detailed":
    # ì¤‘ê°„ ì‘ë‹µ: ìš”ì•½ + ë¼ìš´ë“œë³„ ìš”ì•½
    final_review_text = extract_detailed_review(session, max_tokens=15000)

elif verbosity == "full":
    # ì „ì²´ ì‘ë‹µ: ëª¨ë“  ë‚´ìš© (í† í° ì œí•œ ì´ˆê³¼ ê°€ëŠ¥ì„± ê²½ê³ )
    final_review_text = session.full_transcript
```

### Phase 4: Token Counting & Validation (ìš°ì„ ìˆœìœ„: ë†’ìŒ)

#### 4.1 í† í° ì¹´ìš´í„°
**íŒŒì¼**: `src/mcp/utils/token_counter.py`
```python
import tiktoken

def count_tokens(text: str, model: str = "gpt-4") -> int:
    """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚°"""
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))

def truncate_to_tokens(text: str, max_tokens: int, model: str = "gpt-4") -> str:
    """í…ìŠ¤íŠ¸ë¥¼ ì§€ì • í† í° ìˆ˜ë¡œ ìë¥´ê¸°"""
    encoding = tiktoken.encoding_for_model(model)
    tokens = encoding.encode(text)

    if len(tokens) <= max_tokens:
        return text

    # í† í° ì˜ë¼ë‚´ê¸°
    truncated_tokens = tokens[:max_tokens]
    return encoding.decode(truncated_tokens) + "\n\n...(truncated)"
```

#### 4.2 ì‘ë‹µ ê²€ì¦
```python
def validate_response_size(response: ReviewResponse) -> None:
    """
    ì‘ë‹µ í¬ê¸°ê°€ MCP ì œí•œì„ ì´ˆê³¼í•˜ì§€ ì•ŠëŠ”ì§€ ê²€ì¦

    Raises:
        ValueError: 25,000 í† í° ì´ˆê³¼ì‹œ
    """
    # JSON ì§ë ¬í™” í›„ í† í° ì¹´ìš´íŠ¸
    response_json = json.dumps(response)
    token_count = count_tokens(response_json)

    if token_count > 25000:
        raise ValueError(
            f"Response size ({token_count} tokens) exceeds MCP limit (25000). "
            f"Consider using verbosity='summary' or reducing content."
        )
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤

### Test 1: Summary Response (run_code_review)
```python
async def test_run_code_review_summary_response():
    """run_code_review summary ëª¨ë“œê°€ 5000 í† í° ì´í•˜ì¸ì§€ í™•ì¸"""
    response = await review_run_code_review(
        base="develop",
        target="feature/large-change",
        max_rounds=3,
        verbosity="summary"
    )

    token_count = count_tokens(response["final_review_text"])
    assert token_count <= 5000
    assert response["artifacts"]["summary_file"].endswith(".md")
    assert os.path.exists(response["artifacts"]["summary_file"])
```

### Test 1-2: Summary Response (audit_code_review)
```python
async def test_audit_code_review_summary_response():
    """audit_code_review summary ëª¨ë“œê°€ 5000 í† í° ì´í•˜ì¸ì§€ í™•ì¸"""
    initial_review = """
    # My Review
    - Good code structure
    - Need more tests
    """

    response = await review_audit_code_review(
        base="develop",
        target="feature/auth",
        initial_review=initial_review,
        max_rounds=3,
        verbosity="summary"
    )

    token_count = count_tokens(response["final_review_text"])
    assert token_count <= 5000
    assert response["artifacts"]["summary_file"].endswith(".md")
    assert os.path.exists(response["artifacts"]["summary_file"])

    # auditë§Œ í•´ë‹¹: initial_review.md íŒŒì¼ í™•ì¸
    initial_review_path = os.path.join(
        os.path.dirname(response["artifacts"]["summary_file"]),
        "initial-review.md"
    )
    assert os.path.exists(initial_review_path)
```

### Test 2: File Artifacts
```python
async def test_artifact_generation():
    """ëª¨ë“  ì•„í‹°íŒ©íŠ¸ê°€ ì˜¬ë°”ë¥´ê²Œ ìƒì„±ë˜ëŠ”ì§€ í™•ì¸"""
    response = await review_run_code_review(
        base="develop",
        target="refactor/cleanup",
        max_rounds=2
    )

    artifacts = response["artifacts"]

    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    assert os.path.exists(artifacts["summary_file"])
    assert os.path.exists(artifacts["full_transcript"])
    assert os.path.exists(artifacts["consensus_log"])
    assert os.path.isdir(artifacts["rounds_dir"])

    # ë¼ìš´ë“œ íŒŒì¼ í™•ì¸
    round_files = os.listdir(artifacts["rounds_dir"])
    assert len(round_files) >= 6  # 2 rounds Ã— 3 AIs
```

### Test 3: Token Limit Compliance
```python
async def test_mcp_token_limit():
    """ì „ì²´ ì‘ë‹µì´ MCP ì œí•œì„ ì´ˆê³¼í•˜ì§€ ì•ŠëŠ”ì§€ í™•ì¸"""
    response = await review_run_code_review(
        base="develop",
        target="feature/very-large",
        max_rounds=3,
        verbosity="summary"
    )

    response_json = json.dumps(response)
    token_count = count_tokens(response_json)

    assert token_count <= 25000, f"Response {token_count} tokens exceeds limit"
```

### Test 4: Claude Context Usability (run_code_review)
```python
async def test_claude_can_use_context_run():
    """Claudeê°€ run_code_review ì‘ë‹µì„ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš© ê°€ëŠ¥í•œì§€ í™•ì¸"""
    response = await review_run_code_review(
        base="develop",
        target="feature/auth",
        max_rounds=2
    )

    # ìµœì¢… ë¦¬ë·° í…ìŠ¤íŠ¸ê°€ êµ¬ì¡°í™”ë˜ì–´ ìˆëŠ”ì§€
    assert "Critical Issues" in response["final_review_text"]
    assert "Recommendations" in response["final_review_text"]

    # í•µì‹¬ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€
    assert response["summary"]["critical_issues"] >= 0
    assert len(response["summary"]["key_findings"]) > 0
```

### Test 4-2: Claude Context Usability (audit_code_review)
```python
async def test_claude_can_use_context_audit():
    """Claudeê°€ audit_code_review ì‘ë‹µì„ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš© ê°€ëŠ¥í•œì§€ í™•ì¸"""
    initial_review = "Basic review without security analysis"

    response = await review_audit_code_review(
        base="develop",
        target="feature/payment",
        initial_review=initial_review,
        max_rounds=2
    )

    # audit íŠ¹í™” ë‚´ìš© í™•ì¸
    assert "Audit Findings" in response["final_review_text"] or \
           "Initial Review Assessment" in response["final_review_text"]
    assert "Issues Added by Auditors" in response["final_review_text"] or \
           response["summary"]["critical_issues"] >= 0

    # í•µì‹¬ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€
    assert len(response["summary"]["key_findings"]) > 0
```

### Test 5: Response Structure Consistency
```python
async def test_response_structure_consistency():
    """ë‘ ë„êµ¬ê°€ ë™ì¼í•œ ì‘ë‹µ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ëŠ”ì§€ í™•ì¸"""

    # run_code_review ì‘ë‹µ
    run_response = await review_run_code_review(
        base="develop",
        target="feature/test",
        max_rounds=2
    )

    # audit_code_review ì‘ë‹µ
    audit_response = await review_audit_code_review(
        base="develop",
        target="feature/test",
        initial_review="Test review",
        max_rounds=2
    )

    # ë‘ ì‘ë‹µì˜ êµ¬ì¡°ê°€ ë™ì¼í•œì§€ í™•ì¸
    assert set(run_response.keys()) == set(audit_response.keys())
    assert set(run_response["consensus"].keys()) == set(audit_response["consensus"].keys())
    assert set(run_response["summary"].keys()) == set(audit_response["summary"].keys())
    assert set(run_response["artifacts"].keys()) == set(audit_response["artifacts"].keys())
```

---

## ğŸ“Š ì„±ê³µ ê¸°ì¤€

### í•„ìˆ˜ (Must Have)
- [ ] `review_run_code_review` ì‘ë‹µì´ 25,000 í† í° ì´í•˜
- [ ] `review_audit_code_review` ì‘ë‹µì´ 25,000 í† í° ì´í•˜
- [ ] **ë‘ ë„êµ¬ê°€ ë™ì¼í•œ `ReviewResponse` êµ¬ì¡° ì‚¬ìš©**
- [ ] Claudeê°€ ìµœì¢… ë¦¬ë·°ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš© ê°€ëŠ¥ (ë‘ ë„êµ¬ ëª¨ë‘)
- [ ] ì „ì²´ íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ê°€ íŒŒì¼ë¡œ ë³´ì¡´ (ë‘ ë„êµ¬ ëª¨ë‘)
- [ ] ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼ (ë‘ ë„êµ¬ ëª¨ë‘)

### ê¶Œì¥ (Should Have)
- [ ] `summary.md`ê°€ 5,000 í† í° ì´í•˜
- [ ] íŒŒì¼ êµ¬ì¡°ê°€ ì§ê´€ì 
- [ ] ë©”íƒ€ë°ì´í„°(JSON)ê°€ ì •í™•
- [ ] í† í° ì¹´ìš´íŒ… ì •í™•ë„ 95% ì´ìƒ

### ì„ íƒ (Nice to Have)
- [ ] `verbosity` íŒŒë¼ë¯¸í„°ë¡œ ì‘ë‹µ í¬ê¸° ì¡°ì ˆ
- [ ] ë¼ìš´ë“œë³„ diff í•˜ì´ë¼ì´íŒ…
- [ ] HTML ë¦¬í¬íŠ¸ ìƒì„±
- [ ] ë¦¬ë·° íˆìŠ¤í† ë¦¬ ê´€ë¦¬

---

## ğŸš€ êµ¬í˜„ ìˆœì„œ

1. **Phase 1**: Response structure (1-2ì‹œê°„)
   - ReviewResponse íƒ€ì… ì •ì˜
   - ê¸°ë³¸ ì‘ë‹µ ìƒì„± ë¡œì§ (ë‘ ë„êµ¬ ê³µí†µ)
   - ReviewSessionì— `review_type` í•„ë“œ ì¶”ê°€

2. **Phase 2**: File artifacts (2-3ì‹œê°„)
   - ë””ë ‰í† ë¦¬ ìƒì„±
   - summary.md ì‘ì„± (run/audit í…œí”Œë¦¿ ë¶„ë¦¬)
   - full-transcript.md ì‘ì„±
   - rounds/ íŒŒì¼ ì‘ì„±
   - initial-review.md ì‘ì„± (auditë§Œ í•´ë‹¹)
   - review-type.txt ì‘ì„±

3. **Phase 3**: Token management (1ì‹œê°„)
   - tiktoken í†µí•©
   - í† í° ì¹´ìš´íŒ…
   - Truncation ë¡œì§

4. **Phase 4**: MCP Handler Integration (1ì‹œê°„)
   - `review_run_code_review` ìˆ˜ì •
   - `review_audit_code_review` ìˆ˜ì •
   - ë‘ í•¸ë“¤ëŸ¬ê°€ `create_review_response()` ê³µìœ  í™•ì¸

5. **Phase 5**: Testing (2-3ì‹œê°„)
   - ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (ê³µí†µ ë¡œì§)
   - run_code_review í†µí•© í…ŒìŠ¤íŠ¸
   - audit_code_review í†µí•© í…ŒìŠ¤íŠ¸
   - ì‘ë‹µ êµ¬ì¡° ì¼ê´€ì„± í…ŒìŠ¤íŠ¸
   - ì‹¤ì œ ë¦¬ë·° ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸

**ì˜ˆìƒ ì´ ì†Œìš” ì‹œê°„**: 7-10ì‹œê°„

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [MCP í”„ë¡œí† ì½œ ìŠ¤í™](https://modelcontextprotocol.io/docs)
- [tiktoken ë¬¸ì„œ](https://github.com/openai/tiktoken)
- [AsyncIO íŒŒì¼ I/O](https://github.com/Tinche/aiofiles)

---

**ì‘ì„±ì¼**: 2025-01-01
**ë²„ì „**: 2.0
**ì‘ì„±ì**: Development Team
